{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from itertools import islice\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get path to the dataset from command line\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Information of training set')\n",
    "# parser.add_argument('--path_to_dataset', type=str,\n",
    "#                     help='Path to text file with pairs')\n",
    "# parser.add_argument('--num_samples', type=int,\n",
    "# \thelp='Total number of samples in train and test')\n",
    "# parser.add_argument('--num_epochs', type=int, help='Number of epochs for train')\n",
    "# parser.add_argument('--path_to_model', type=str,\n",
    "#     help='Path to file with neural network weights')\n",
    "# args = parser.parse_args()\n",
    "# print(args)\n",
    "\n",
    "\n",
    "# --path_to_dataset ./data/fra-eng/transcriptions_data.txt \n",
    "# --num_samples 5000 --path_to_model ./data/fra-eng/model_test_transcriptions_test.hdf5 --num_epochs 1\n",
    "\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.path_to_dataset = './data/fra-eng/transcriptions_data.txt'\n",
    "args.num_samples = 5000000\n",
    "args.path_to_model = './data/fra-eng/model_transcriptions_test_test_test.hdf5'\n",
    "args.num_epochs = 1\n",
    "\n",
    "# Network variables\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = args.num_epochs  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "data_path = args.path_to_dataset\n",
    "path_to_train = \"./data/fra-eng/train_transcriptions.txt\"\n",
    "path_to_val = \"./data/fra-eng/val_transcriptions.txt\"\n",
    "path_to_test = \"./data/fra-eng/test_transcriptions.txt\"\n",
    "\n",
    "# Set total number of samples\n",
    "\n",
    "if not args.num_samples:\n",
    "\tnum_samples = sum(1 for line in open(data_path))\n",
    "else:\n",
    "\tnum_samples = args.num_samples\n",
    "\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_session(gpu_fraction=0.01):\n",
    "    '''Assume that you have 6GB of GPU memory and want to allocate ~2GB'''\n",
    "\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "KTF.set_session(get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique characters for input and target\n",
    "\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "max_encoder_seq_length = 0\n",
    "max_decoder_seq_length = 0\n",
    "with open(data_path) as file:\n",
    "    for line, i in zip(file, range(num_samples)):\n",
    "        input_line, target_line = line.split('\\t')\n",
    "        # We use \"tab\" as the \"start sequence\" character\n",
    "        # We use \"\\n\" as the \"end sequence\" character\n",
    "        target_line = \"\\t\" + target_line + \"\\n\"\n",
    "        if len(input_line) > max_encoder_seq_length:\n",
    "            max_encoder_seq_length = len(input_line)\n",
    "        if len(target_line) > max_decoder_seq_length:\n",
    "            max_decoder_seq_length = len(target_line)\n",
    "        for char in input_line:\n",
    "            if char not in input_characters:\n",
    "                input_characters.add(char)\n",
    "        for char in target_line:\n",
    "            if char not in target_characters:\n",
    "                target_characters.add(char)\n",
    "\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max encoder sequence lenght: 36\n",
      "max decoder sequence lenght:  37\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "print('max encoder sequence lenght:', max_encoder_seq_length)\n",
    "print('max decoder sequence lenght: ', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# np.save(\"./data/fra-eng/input_tokens.npy\", input_token_index)\n",
    "# np.save(\"./data/fra-eng/target_tokens.npy\", target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset for train and test\n",
    "\n",
    "def train_test_split(path_to_data=\"./data/fra-eng/fra.txt\",\n",
    "                     path_to_train=\"./data/fra-eng/train.txt\",\n",
    "                     path_to_val=\"./data/fra-eng/val.txt\",\n",
    "                     path_to_test=\"./data/fra-eng/test.txt\",\n",
    "                     random_seed=42,\n",
    "                     num_samples=10000, train_size=0.8, val_size=0.1):\n",
    "    # Create files\n",
    "    print('------------')\n",
    "    print('PATH TO DATASET:', path_to_data)\n",
    "    print('PATH TO TRAIN SET:', path_to_train)\n",
    "    print('PATH TO VAL SET:', path_to_val)\n",
    "    print('PATH TO TEST SET:', path_to_test)\n",
    "    print('RANDOM SEED', random_seed)\n",
    "    print('------------')\n",
    "    with open(path_to_train, 'w+') as train_file, \\\n",
    "            open(path_to_val, 'w+') as val_file, \\\n",
    "            open(path_to_test, 'w+') as test_file, \\\n",
    "                open(data_path, 'r') as dataset:\n",
    "                    # create binary mask to determine whether line is in\n",
    "                    # train or test set\n",
    "                    \n",
    "                    number_of_train_samples = int(num_samples * train_size)\n",
    "                    number_of_val_samples = int(num_samples * val_size)\n",
    "                    number_of_test_samples = num_samples - (number_of_train_samples + number_of_val_samples)\n",
    "                    \n",
    "#                     train_inds = np.random.choice(np.arange(num_samples), number_of_train_samples, replace=False)\n",
    "#                     test_inds = np.setdiff1d(np.arange(num_samples), train_inds)\n",
    "#                     val_inds = np.random.choice(test_inds, number_of_val_samples, replace=False)\n",
    "#                     test_inds = np.setdiff1d(test_inds, val_inds)\n",
    "                    \n",
    "#                     print (len(train_inds), len(val_inds), len(test_inds))\n",
    "                    \n",
    "#                     binary_array = np.append(\n",
    "#                         np.ones(number_of_train_samples),\n",
    "#                         np.zeros(number_of_test_samples))\n",
    "#                     random.shuffle(binary_array)\n",
    "#                     for line, is_train_sample in zip(dataset, binary_array):\n",
    "#                         if is_train_sample:\n",
    "#                             train_file.write(line)\n",
    "#                         else:\n",
    "#                             test_file.write(line)\n",
    "                    last_val_word = ''\n",
    "                    for line, ind in tqdm(zip(dataset, range(num_samples))):\n",
    "                        if ind <= number_of_train_samples:\n",
    "                            # print ('hello')\n",
    "                            train_file.write(line)\n",
    "                            last_word = line.split('\\t')[1]\n",
    "                        else:\n",
    "                            if last_word == line.split('\\t')[1]:\n",
    "                                # print ('nu privet', ind, line)\n",
    "                                continue\n",
    "                            elif ind <= number_of_train_samples + number_of_val_samples:\n",
    "                                # print ('ya zdes', line)\n",
    "                                val_file.write(line)\n",
    "                                last_val_word = line.split('\\t')[1]\n",
    "                                print (last_val_word)\n",
    "                            \n",
    "                            else:\n",
    "                                if last_val_word == line.split('\\t')[1]:\n",
    "                                    # print ('hello while while while')\n",
    "                                    continue\n",
    "                                # print ('e boooooy kkk bbrrrraaaaaaa')\n",
    "                                test_file.write(line)\n",
    "                        \n",
    "    train_file.close()\n",
    "    val_file.close()\n",
    "    test_file.close()\n",
    "    dataset.close()\n",
    "\n",
    "    # delete last empty lines from train and test\n",
    "    # delete_last_line(path_to_train)\n",
    "    # delete_last_line(path_to_test)\n",
    "\n",
    "    return (train_size, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:00, 185920.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "PATH TO DATASET: ./data/fra-eng/transcriptions_data.txt\n",
      "PATH TO TRAIN SET: ./data/fra-eng/train_transcriptions.txt\n",
      "PATH TO VAL SET: ./data/fra-eng/val_transcriptions.txt\n",
      "PATH TO TEST SET: ./data/fra-eng/test_transcriptions.txt\n",
      "RANDOM SEED 42\n",
      "------------\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "risi\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "troʊlz\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "læboʊ\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "vɑlɛnti\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n",
      "ketidɪd\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_size, val_size = train_test_split(path_to_data=data_path, num_samples=num_samples,\n",
    "                 path_to_train=path_to_train, path_to_val=path_to_val, path_to_test=path_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7211241"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for line in open(args.path_to_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2401"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for line in open(path_to_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yau\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_val) as file:\n",
    "    for line in file:\n",
    "        word = line.split('\\t')[1]\n",
    "        with open(path_to_train) as train:\n",
    "            for train_line in train:\n",
    "                if word == train_line.split('\\t')[1]:\n",
    "                    print ('blyyyyaaa <3 train', word)\n",
    "        with open(path_to_test) as test:\n",
    "            for test_line in test:\n",
    "                if word == test_line.split('\\t')[1]:\n",
    "                    print ('blyyyyaaa <3 test', word)\n",
    "print ('yau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699978"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for line in open(path_to_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699930"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for line in open(path_to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure architecture for networks\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Additional reading about hidden and cell states\n",
    "https://www.quora.com/How-is-the-hidden-state-h-different-from-the-memory-c-in-an-LSTM-cell\n",
    "'''\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data generator\n",
    "\n",
    "\n",
    "class DataGenerator():\n",
    "    def __init__(self, max_encoder_sequence_lenght, max_decoder_sequence_lenght,\n",
    "                 path_to_file='./data/fra-eng/fra.txt',\n",
    "                 batch_size=64, train_samples_count=float('inf'),\n",
    "                 path_to_input_tokens_dict=\"./data/fra-eng/input_tokens.npy\",\n",
    "                 path_to_target_tokens_dict=\"./data/fra-eng/target_tokens.npy\"):\n",
    "        # Initialization\n",
    "        self.path_to_file = path_to_file\n",
    "        self.batch_size = batch_size\n",
    "        self.train_samples_count = min(self.count_lines(), train_samples_count)\n",
    "        self.max_encoder_seq_length = max_encoder_sequence_lenght\n",
    "        self.max_decoder_seq_length = max_decoder_sequence_lenght\n",
    "        self.input_token_index = np.load(path_to_input_tokens_dict).item()\n",
    "        self.target_token_index = np.load(path_to_target_tokens_dict).item()\n",
    "        self.lines_beginnings = self.get_lines_beginnings()\n",
    "\n",
    "        print('-------------------------')\n",
    "        print('PATH TO FILE:', self.path_to_file)\n",
    "        print('BATCH SIZE:', self.batch_size)\n",
    "        print('NUMBER OF SAMPLES IN FILE', self.train_samples_count)\n",
    "        print('MAX ENCODER SEQUENCE LENGHT', self.max_encoder_seq_length)\n",
    "        print('MAX DECODER SEQUENCE LENGHT', self.max_decoder_seq_length)\n",
    "        print('LEN OF TARGET TOKENS DICT', len(self.target_token_index))\n",
    "        print('LEN OF INPUT TOKENS DICT', len(self.input_token_index))\n",
    "        print('--------------------------')\n",
    "\n",
    "    def get_lines_beginnings(self):\n",
    "        print ('start counting lines')\n",
    "        # returns an array of positions of lines beginnings\n",
    "        positions = []\n",
    "        with open(self.path_to_file) as file:\n",
    "            line = file.readline()\n",
    "            while line:\n",
    "                positions.append(file.tell())\n",
    "                line = file.readline()\n",
    "        print ('finished counting')\n",
    "        return positions\n",
    "\n",
    "    def sliding_window(self, arr, window_size=5, step=2):\n",
    "        iterator = iter(arr)\n",
    "        window = []\n",
    "        for element in range(window_size):\n",
    "            window.append(next(iterator))\n",
    "        yield window\n",
    "        while True:\n",
    "            for i in range(step):\n",
    "                window = window[1:] + [next(iterator)]\n",
    "            yield window\n",
    "\n",
    "    def count_lines(self):\n",
    "        return sum(1 for line in open(self.path_to_file))\n",
    "\n",
    "    def generate(self):\n",
    "        while 1:\n",
    "            with open(self.path_to_file, 'r') as dataset:\n",
    "                '''\n",
    "                Shuffle pointers to the beginnings of lines to\n",
    "                have different elements in batches every epoch\n",
    "                '''\n",
    "                random.shuffle(self.lines_beginnings)\n",
    "                for positions in self.sliding_window(self.lines_beginnings,\n",
    "                                                 window_size=self.batch_size, step=self.batch_size):\n",
    "                    input_texts = []\n",
    "                    target_texts = []\n",
    "                    for byte_index in positions:\n",
    "                        dataset.seek(byte_index)\n",
    "                        pair = dataset.readline()\n",
    "                        # Handle messed up lines from dataset\n",
    "                        try:\n",
    "                            input_text, target_text = pair.split(\"\\t\")\n",
    "                        except ValueError:\n",
    "                            print(\n",
    "    \"\\n INCONSISTENT INPUT:\",\n",
    "    pair,\n",
    "    'at index',\n",
    "     byte_index)\n",
    "                            continue\n",
    "                        # \"tab\" is the \"start sequence\" character\n",
    "                        # \"\\n\" is the \"end sequence\" character\n",
    "                        target_text = \"\\t\" + target_text + \"\\n\"\n",
    "                        input_texts.append(input_text)\n",
    "                        target_texts.append(target_text)\n",
    "                    # initialize placeholders for vectors\n",
    "                    encoder_input_data = np.zeros(\n",
    "                        (len(input_texts),\n",
    "    self.max_encoder_seq_length,\n",
    "     num_encoder_tokens),\n",
    "                        dtype='float32')\n",
    "                    decoder_input_data = np.zeros(\n",
    "                        (len(input_texts),\n",
    "    self.max_decoder_seq_length,\n",
    "     num_decoder_tokens),\n",
    "                        dtype='float32')\n",
    "                    decoder_target_data = np.zeros(\n",
    "                        (len(input_texts),\n",
    "    self.max_decoder_seq_length,\n",
    "     num_decoder_tokens),\n",
    "                        dtype='float32')\n",
    "                    for i, (input_text, target_text) in enumerate(\n",
    "                        zip(input_texts, target_texts)):\n",
    "                        for t, char in enumerate(input_text):\n",
    "                            encoder_input_data[i, t,\n",
    "                                self.input_token_index[char]] = 1.\n",
    "                        for t, char in enumerate(target_text):\n",
    "                            # decoder_target_data is ahead of decoder_input_data\n",
    "                            # by one timestep\n",
    "                            decoder_input_data[i, t, self.target_token_index[char]] = 1.\n",
    "                            if t > 0:\n",
    "                                # decoder_target_data will be ahead by one timestep\n",
    "                                # and will not include the start character.\n",
    "                                decoder_target_data[i, t - 1, self.target_token_index[char]] = 1.\n",
    "                    yield ([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "            dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-77c827ccd3ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'seq2seq'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m train_generator = DataGenerator(max_encoder_sequence_lenght=max_encoder_seq_length, \n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the model\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq')\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "train_generator = DataGenerator(max_encoder_sequence_lenght=max_encoder_seq_length, \n",
    "                                max_decoder_sequence_lenght=max_decoder_seq_length,\n",
    "                                path_to_file=path_to_train)\n",
    "print ('train initialized')\n",
    "validation_generator = DataGenerator(max_encoder_sequence_lenght=max_encoder_seq_length, \n",
    "                                     max_decoder_sequence_lenght=max_decoder_seq_length,\n",
    "                                     path_to_file=path_to_val)\n",
    "\n",
    "if not args.path_to_model:\n",
    "    path_to_model = \"./data/fra-eng/fra.hdf5\"\n",
    "else:\n",
    "    path_to_model = args.path_to_model\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=path_to_model+\".{epoch:02d}-{val_loss:.2f}.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "class OnEpochEndCallback(Callback):\n",
    "    \"\"\"Execute this every end of epoch\"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"On Epoch end - do some stats\"\"\"\n",
    "        test_model(self.model)\n",
    "\n",
    "ON_EPOCH_END_CALLBACK = OnEpochEndCallback()\n",
    "\n",
    "model.fit_generator(train_generator.generate(), \n",
    "                    steps_per_epoch=train_generator.train_samples_count // train_generator.batch_size, \n",
    "                    epochs = epochs,\n",
    "                    validation_data=validation_generator.generate(), \n",
    "                    validation_steps=validation_generator.train_samples_count // validation_generator.batch_size,\n",
    "                    callbacks=[checkpointer, ON_EPOCH_END_CALLBACK, ])\n",
    "\n",
    "\n",
    "# os.system('touch ' + path_to_model)\n",
    "model.save_weights(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, reverse_input_char_index, reverse_target_char_index):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "def test_model(model):\n",
    "    # Define sampling models\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # Reverse-lookup token index to decode sequences back to\n",
    "    # something readable.\n",
    "    reverse_input_char_index = dict(\n",
    "        (i, char) for char, i in input_token_index.items())\n",
    "    reverse_target_char_index = dict(\n",
    "        (i, char) for char, i in target_token_index.items())\n",
    "    \n",
    "    number_of_test_samples = 10\n",
    "    number_of_correct_answers = 0\n",
    "    \n",
    "    lines = open(path_to_test).readlines()\n",
    "    \n",
    "    for i in tqdm(range(number_of_test_samples)):\n",
    "        # lines = open(path_to_test).read()\n",
    "        line = random.choice(lines)\n",
    "\n",
    "        input_text, target_text = line.split(\"\\t\")\n",
    "        print ('--------')\n",
    "        print ('Input sentence:', input_text)\n",
    "        encoder_input_data = np.zeros(\n",
    "                        (1, max_encoder_seq_length, num_encoder_tokens),\n",
    "                        dtype='float32')\n",
    "        for t, char in enumerate(input_text):\n",
    "                            encoder_input_data[0, t, input_token_index[char]] = 1.\n",
    "        decoded_sentence = decode_sequence(encoder_input_data, encoder_model, decoder_model, \\\n",
    "                                          reverse_input_char_index, reverse_target_char_index)\n",
    "        print ('Decoded sentence:', decoded_sentence)\n",
    "        print ('Correct answer:', target_text)\n",
    "        if decoded_sentence == target_text:\n",
    "            number_of_correct_answers += 1\n",
    "        \n",
    "        print ('Number of correct answers:', number_of_correct_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test excisting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq')\n",
    "\n",
    "model.load_weights(\"./data/fra-eng/model_transcriptions_test_weekend.hdf5\") \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 822/699930 [00:50<11:49:30, 16.42it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/500 [00:00<01:28,  5.65it/s]\u001b[A\n",
      "  1%|          | 4/500 [00:00<00:35, 13.96it/s]\u001b[A\n",
      "  1%|▏         | 7/500 [00:00<00:27, 17.70it/s]\u001b[A\n",
      "  2%|▏         | 10/500 [00:00<00:26, 18.67it/s]\u001b[A\n",
      "  3%|▎         | 14/500 [00:00<00:24, 20.22it/s]\u001b[A\n",
      "  3%|▎         | 17/500 [00:00<00:23, 20.53it/s]\u001b[A\n",
      "  4%|▍         | 20/500 [00:00<00:22, 21.04it/s]\u001b[A\n",
      "  5%|▍         | 23/500 [00:01<00:22, 21.34it/s]\u001b[A\n",
      "  5%|▌         | 26/500 [00:01<00:22, 21.26it/s]\u001b[A\n",
      "  6%|▌         | 29/500 [00:01<00:21, 21.76it/s]\u001b[A\n",
      "  6%|▋         | 32/500 [00:01<00:21, 21.99it/s]\u001b[A\n",
      "  7%|▋         | 35/500 [00:01<00:21, 22.00it/s]\u001b[A\n",
      "  8%|▊         | 38/500 [00:01<00:20, 22.16it/s]\u001b[A\n",
      "  8%|▊         | 41/500 [00:01<00:20, 22.36it/s]\u001b[A\n",
      "  9%|▉         | 44/500 [00:01<00:20, 22.32it/s]\u001b[A\n",
      "  9%|▉         | 47/500 [00:02<00:20, 22.18it/s]\u001b[A\n",
      " 10%|█         | 50/500 [00:02<00:20, 22.34it/s]\u001b[A\n",
      " 11%|█         | 53/500 [00:02<00:20, 22.32it/s]\u001b[A\n",
      " 11%|█         | 56/500 [00:02<00:19, 22.53it/s]\u001b[A\n",
      " 12%|█▏        | 59/500 [00:02<00:19, 22.48it/s]\u001b[A\n",
      " 12%|█▏        | 62/500 [00:02<00:19, 22.71it/s]\u001b[A\n",
      " 13%|█▎        | 65/500 [00:02<00:19, 22.80it/s]\u001b[A\n",
      " 14%|█▎        | 68/500 [00:02<00:18, 22.95it/s]\u001b[A\n",
      " 14%|█▍        | 71/500 [00:03<00:18, 22.93it/s]\u001b[A\n",
      " 15%|█▍        | 74/500 [00:03<00:18, 22.90it/s]\u001b[A\n",
      " 15%|█▌        | 77/500 [00:03<00:18, 22.91it/s]\u001b[A\n",
      " 16%|█▌        | 80/500 [00:03<00:18, 23.07it/s]\u001b[A\n",
      " 17%|█▋        | 83/500 [00:03<00:18, 23.01it/s]\u001b[A\n",
      " 17%|█▋        | 86/500 [00:03<00:17, 23.05it/s]\u001b[A\n",
      " 18%|█▊        | 89/500 [00:03<00:17, 23.14it/s]\u001b[A\n",
      " 18%|█▊        | 92/500 [00:03<00:17, 23.16it/s]\u001b[A\n",
      " 19%|█▉        | 95/500 [00:04<00:17, 23.21it/s]\u001b[A\n",
      " 20%|█▉        | 98/500 [00:04<00:17, 23.20it/s]\u001b[A\n",
      " 20%|██        | 101/500 [00:04<00:17, 23.18it/s]\u001b[A\n",
      " 21%|██        | 104/500 [00:04<00:17, 23.17it/s]\u001b[A\n",
      " 21%|██▏       | 107/500 [00:04<00:16, 23.17it/s]\u001b[A\n",
      " 22%|██▏       | 110/500 [00:04<00:16, 23.27it/s]\u001b[A\n",
      " 23%|██▎       | 113/500 [00:04<00:16, 23.36it/s]\u001b[A\n",
      " 23%|██▎       | 116/500 [00:04<00:16, 23.36it/s]\u001b[A\n",
      " 24%|██▍       | 119/500 [00:05<00:16, 23.33it/s]\u001b[A\n",
      " 24%|██▍       | 122/500 [00:05<00:16, 23.39it/s]\u001b[A\n",
      " 25%|██▌       | 125/500 [00:05<00:15, 23.48it/s]\u001b[A\n",
      " 26%|██▌       | 128/500 [00:05<00:15, 23.48it/s]\u001b[A\n",
      " 26%|██▌       | 131/500 [00:05<00:15, 23.51it/s]\u001b[A\n",
      " 27%|██▋       | 134/500 [00:05<00:15, 23.40it/s]\u001b[A\n",
      " 27%|██▋       | 137/500 [00:05<00:15, 23.43it/s]\u001b[A\n",
      " 28%|██▊       | 140/500 [00:05<00:15, 23.43it/s]\u001b[A\n",
      " 29%|██▊       | 143/500 [00:06<00:15, 23.48it/s]\u001b[A\n",
      " 29%|██▉       | 146/500 [00:06<00:15, 23.52it/s]\u001b[A\n",
      " 30%|██▉       | 149/500 [00:06<00:14, 23.58it/s]\u001b[A\n",
      " 30%|███       | 152/500 [00:06<00:14, 23.56it/s]\u001b[A\n",
      " 31%|███       | 155/500 [00:06<00:14, 23.52it/s]\u001b[A\n",
      " 32%|███▏      | 158/500 [00:06<00:14, 23.51it/s]\u001b[A\n",
      " 32%|███▏      | 161/500 [00:06<00:14, 23.51it/s]\u001b[A\n",
      " 33%|███▎      | 164/500 [00:06<00:14, 23.49it/s]\u001b[A\n",
      " 33%|███▎      | 167/500 [00:07<00:14, 23.46it/s]\u001b[A\n",
      " 34%|███▍      | 170/500 [00:07<00:14, 23.54it/s]\u001b[A\n",
      " 35%|███▍      | 173/500 [00:07<00:13, 23.59it/s]\u001b[A\n",
      " 35%|███▌      | 176/500 [00:07<00:13, 23.65it/s]\u001b[A\n",
      " 36%|███▌      | 179/500 [00:07<00:13, 23.69it/s]\u001b[A\n",
      " 36%|███▋      | 182/500 [00:07<00:13, 23.65it/s]\u001b[A\n",
      " 37%|███▋      | 185/500 [00:07<00:13, 23.68it/s]\u001b[A\n",
      " 38%|███▊      | 188/500 [00:07<00:13, 23.70it/s]\u001b[A\n",
      " 38%|███▊      | 191/500 [00:08<00:13, 23.75it/s]\u001b[A\n",
      " 39%|███▉      | 194/500 [00:08<00:12, 23.68it/s]\u001b[A\n",
      " 39%|███▉      | 197/500 [00:08<00:12, 23.66it/s]\u001b[A\n",
      " 40%|████      | 200/500 [00:08<00:12, 23.62it/s]\u001b[A\n",
      " 41%|████      | 203/500 [00:08<00:12, 23.58it/s]\u001b[A\n",
      " 41%|████      | 206/500 [00:08<00:12, 23.59it/s]\u001b[A\n",
      " 42%|████▏     | 209/500 [00:08<00:12, 23.62it/s]\u001b[A\n",
      " 42%|████▏     | 212/500 [00:08<00:12, 23.60it/s]\u001b[A\n",
      " 43%|████▎     | 215/500 [00:09<00:12, 23.63it/s]\u001b[A\n",
      " 44%|████▎     | 218/500 [00:09<00:11, 23.66it/s]\u001b[A\n",
      " 44%|████▍     | 221/500 [00:09<00:11, 23.69it/s]\u001b[A\n",
      " 45%|████▍     | 224/500 [00:09<00:11, 23.68it/s]\u001b[A\n",
      " 45%|████▌     | 227/500 [00:09<00:11, 23.67it/s]\u001b[A\n",
      " 46%|████▌     | 230/500 [00:09<00:11, 23.72it/s]\u001b[A\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/esvitank/liza/.new/lib/python3.4/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/esvitank/liza/.new/lib/python3.4/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      " 47%|████▋     | 233/500 [00:09<00:11, 23.72it/s]\n",
      "100%|██████████| 500/500 [00:20<00:00, 23.88it/s]\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.608"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
